{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probit Maximum Likelihood\n",
    "\n",
    "In this homework you should implement the maximum likelihood estimator for the probit model. To remind you, this model is defined as follows:\n",
    "    $$\n",
    "    \\begin{align}  \n",
    "    y_i  &\\in \\{0,1\\} \\\\\n",
    "    \\Pr\\{y_i=1\\} &= \\Phi(x_i \\beta) \\\\\n",
    "    L(\\beta)   & = \\Pi_{i=1}^N  \\Phi(x_i \\beta)^{y_i} (1-\\Phi(x_i \\beta))^{1-y_i} \\\\\n",
    "    \\beta  & \\in \\mathbb{R}^k \\\\\n",
    "    x_i  & \\sim N\\left([0,0,0],\\left[ \\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{array} \\right] \\right) \\\\\n",
    "    k & = 3 \n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n",
    "Where $\\Phi$ is the standard Normal cdf. Think of $x_i$ as a row-vector. You should proceed as follows:\n",
    "\n",
    "1. define a data generating function with default argument `N=10000`, generating `N` simulated data points from this model. Generate the data using $\\beta=[1,1.5,-0.5]$. The function should return a `Dict` as outlined in the code.\n",
    "1. Define the log likelihood function, $l(\\beta) = \\log(L)$\n",
    "1. Write a function `plotLike` to plot the log likelihood function for different parameter values. Follow the outline of that function.\n",
    "1. Define the function `maximize_like`. this should optimize your log likelihood function.\n",
    "1. (Optional) Define the gradient of the log likelihood function and use it in another optimization `maximize_ike_grad`.\n",
    "1. (Optional) Define the hessian of the log likelihood function and use it in another optimization `maximize_like_grad_hess`.\n",
    "1. (Optional) Use the hessian of the log likelihood function to compute the standard errors of your estimates and use it in `maximize_like_grad_se`\n",
    "\n",
    "## Tests\n",
    "\n",
    "* The code comes with a test suite that you should fill out. \n",
    "* There are some example tests, you should make those work and maybe add other ones. \n",
    "* Please do not change anything in the file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition makeData("
     ]
    },
    {
     "data": {
      "text/plain": [
       "3×1000 Array{Float64,2}:\n",
       " -0.608831  -0.866407   0.621026  …   1.04046     1.19409   -1.05164\n",
       "  1.58287   -0.236586  -1.05025      -0.0916382   0.312285   2.06581\n",
       " -1.44994    1.97725   -0.152985      0.0119036  -0.229407  -1.3653 "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ") in module Main at In[143]:5 overwritten at In[144]:5.\n",
      "WARNING: Method definition makeData(Any) in module Main at In[143]:5 overwritten at In[144]:5.\n",
      "WARNING: Method definition loglik(Array{T<:Any, 1}, Base.Dict) in module Main at In[143]:29 overwritten at In[144]:29.\n",
      "WARNING: Method definition plotLike() in module Main at In[143]:35 overwritten at In[144]:35.\n"
     ]
    }
   ],
   "source": [
    "using Distributions\n",
    "using Gadfly\n",
    "\n",
    "function makeData(n=1000)\n",
    "\t\tk = 3\n",
    "\t\tbeta = [ 1; 1.5; -0.5 ]\n",
    "\t\tmu = zeros(k)\n",
    "\t\tsig = eye(k)\n",
    "\t\tdistrib_x = MvNormal(mu, sig)\n",
    "\t\tglobal X = rand(distrib_x, n)\n",
    "\t\tXbeta = transpose(X)*beta # matrix of X'beta\n",
    "\t\tdistrib = Normal(0,1) # Define the normal distribution law\n",
    "\t\tG = cdf(distrib, Xbeta) # the vector of n elements Pr(y=1)=Psi(X'beta)\n",
    "        global y = ones(n) # defining the vector y as: y=1 when Pr(y=1)>=1/2\n",
    "\t\tfor i in 1:n\n",
    "\t\t\ty[i] =\n",
    "\t\t\tif G[i] >= .5\n",
    "\t\t\t\t1\n",
    "\t\t\telse\n",
    "\t\t\t\t0\n",
    "\t\t\tend\n",
    "\t\tend\n",
    "\t\tDict([(\"betas\", beta), (\"numobs\", n), (\"X\", X), (\"y\", y),  (\"distrib\", distrib)])\n",
    "\tend\n",
    "\n",
    "\t# log likelihood function at x\n",
    "\t# function loglik(betas::Vector,X::Matrix,y::Vector,distrib::UnivariateDistribution)\n",
    "\tfunction loglik(betas::Vector,d::Dict)\n",
    "\t\tPsi = cdf(d[\"distrib\"], transpose(d[\"X\"])*betas)\n",
    "\t\tL = prod(Psi[i]^d[\"y\"][i]*(1-Psi[i])^(1-d[\"y\"][i]) for i in 1:d[\"numobs\"])\n",
    "\t\tlog(L)\n",
    "\tend\n",
    "\n",
    "function plotLike()\n",
    "\t\tf1(beta1::Float64) = loglik([beta1; 1.5; -0.5 ], makeData())\n",
    "\t\tf2(beta2::Float64) = loglik([1.0;beta2;-.5], makeData())\n",
    "\t\tf3(beta3::Float64) = loglik([1.0;1.5;beta3], makeData())\n",
    "\t\tfigure1 = Gadfly.plot(f1, 0.0, 2.0, Guide.xlabel(\"beta1\"), Guide.ylabel(\"Loglikelihood\"), Guide.Title(\"Loglikelihood changing beta1\"))\n",
    "\t\tfigure2 = Gadfly.plot(f2, .5, 2.5, Guide.xlabel(\"beta2\"), Guide.ylabel(\"Loglikelihood\"), Guide.Title(\"Loglikelihood changing beta2\"))\n",
    "\t\tfigure3 = Gadfly.plot(f3, -1.5, .5, Guide.xlabel(\"beta3\"), Guide.ylabel(\"Loglikelihood\"), Guide.Title(\"Loglikelihood changing beta3\"))\n",
    "\t\tdisplay(figure1)\n",
    "\t\tdisplay(figure2)\n",
    "\t\tdisplay(figure3)\n",
    "\tend\n",
    "        \n",
    "makeData()[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
